# EdgeAI for Beginners: Learning Paths and Study Schedule

### Concentrated Learning Path (1 week)

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 0 | Module 0: Introduction to EdgeAI | 1-2 hours |
| Day 1 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 2 | Module 2: SLM Foundations | 3 hours |
| Day 3 | Module 3: SLM Deployment | 2 hours |
| Day 4-5 | Module 4: Model Optimization (6 frameworks) | 4 hours |
| Day 6 | Module 5: SLMOps | 3 hours |
| Day 7 | Module 6-7: AI Agents & Development Tools | 4 hours |
| Day 8 | Module 8: Foundry Local Toolkit (Modern Implementation) | 1 hour |

### Concentrated Learning Path (2 weeks)

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 1-2 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 3-4 | Module 2: SLM Foundations | 3 hours |
| Day 5-6 | Module 3: SLM Deployment | 2 hours |
| Day 7-8 | Module 4: Model Optimization | 4 hours |
| Day 9-10 | Module 5: SLMOps | 3 hours |
| Day 11-12 | Module 6: AI Agents | 2 hours |
| Day 13-14 | Module 7: Development Tools | 3 hours |

### Part-time Study (4 weeks)

| Week | Focus | Estimated Hours |
|------|-------|------------------|
| Week 1 | Module 1-2: Fundamentals & SLM Foundations | 6 hours |
| Week 2 | Module 3-4: Deployment & Optimization | 6 hours |
| Week 3 | Module 5-6: SLMOps & AI Agents | 5 hours |
| Week 4 | Module 7: Development Tools & Integration | 3 hours |

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 0 | Module 0: Introduction to EdgeAI | 1-2 hours |
| Day 1-2 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 3-4 | Module 2: SLM Foundations | 3 hours |
| Day 5-6 | Module 3: SLM Deployment | 2 hours |
| Day 7-8 | Module 4: Model Optimization | 4 hours |
| Day 9-10 | Module 5: SLMOps | 3 hours |
| Day 11-12 | Module 6: SLM Agentic Systems | 2 hours |
| Day 13-14 | Module 7: EdgeAI Implementation Samples | 2 hours |

| Module | Completion Date | Hours Spent | Key Takeaways |
|--------|----------------|-------------|--------------|
| Module 0: Introduction to EdgeAI | | |Edge AI refers to the deployment and execution of artificial intelligence algorithms directly on edge devices—the physical hardware that exists at the "edge" of the network, close to where data is generated and collected |
| Module 1: EdgeAI Fundamentals | | | |
| Module 2: SLM Foundations | | | |
| Module 3: SLM Deployment | | | |
| Module 4: Model Optimization (6 frameworks) | | | |
| Module 5: SLMOps | | | |
| Module 6: SLM Agentic Systems | | | |
| Module 7: EdgeAI Implementation Samples | | | |
| Hands-on Exercises | | | |
| Mini-Project | | | |

### Part-time Study (4 weeks)

| Week | Focus | Estimated Hours |
|------|-------|------------------|
| Week 1 | Module 1-2: Fundamentals & SLM Foundations | 6 hours |
| Week 2 | Module 3-4: Deployment & Optimization | 6 hours |
| Week 3 | Module 5-6: SLMOps & AI Agents | 5 hours |
| Week 4 | Module 7: Development Tools & Integration | 3 hours |

## Introduction

Welcome to the EdgeAI for Beginners study guide! This document is designed to help you navigate the course materials effectively and maximize your learning experience. It provides structured learning paths, suggested study schedules, key concept summaries, and supplementary resources to deepen your understanding of Edge AI technologies.

This is a concise 20-hour course that delivers essential knowledge about EdgeAI in a time-efficient format, making it perfect for busy professionals and students who want to quickly gain practical skills in this emerging field.

## Course Overview

This course is organized into eight comprehensive modules:

0. **Introduction to EdgeAI** - Foundation and context setting with industry applications and learning objectives
1. **EdgeAI Fundamentals and Transformation** - Understanding the core concepts and technology shift
2. **Small Language Model Foundations** - Exploring various SLM families and their architectures
3. **Small Language Model Deployment** - Implementing practical deployment strategies
4. **Model Format Conversion and Quantization** - Advanced optimization with 6 frameworks including OpenVINO
5. **SLMOps - Small Language Model Operations** - Production lifecycle management and deployment
6. **SLM Agentic Systems** - AI agents, function calling, and Model Context Protocol
7. **EdgeAI Implementation Samples** - AI Toolkit, Windows development, and platform-specific implementations
8. **Microsoft Foundry Local – Complete Developer Toolkit** - Local-first development with hybrid Azure integration (Module 08)

## How to Use This Study Guide

- **Progressive Learning**: Follow the modules in order for the most coherent learning experience
- **Knowledge Checkpoints**: Use the self-assessment questions after each section
- **Hands-on Practice**: Complete the suggested exercises to reinforce theoretical concepts
- **Supplementary Resources**: Explore additional materials for topics that interest you most

## Study Schedule Recommendations

### Concentrated Learning Path (1 week)

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 0 | Module 0: Introduction to EdgeAI | 1-2 hours |
| Day 1-2 | Module 1: EdgeAI Fundamentals | 6 hours |
| Day 3-4 | Module 2: SLM Foundations | 8 hours |
| Day 5 | Module 3: SLM Deployment | 3 hours |
| Day 6 | Module 8: Foundry Local Toolkit | 3 hours |

### Part-time Study (3 weeks)

| Week | Focus | Estimated Hours |
|------|-------|------------------|
| Week 1 | Module 0: Introduction + Module 1: EdgeAI Fundamentals | 7-9 hours |
| Week 2 | Module 2: SLM Foundations | 7-8 hours |
| Week 3 | Module 3: SLM Deployment (3h) + Module 8: Foundry Local Toolkit (2-3h) | 5-6 hours |

## Module 0: Introduction to EdgeAI

### Key Learning Objectives

- Understand what Edge AI is and why it matters in today's technology landscape
- Identify major industries transformed by Edge AI and their specific use cases
- Comprehend the advantages of Small Language Models (SLMs) for edge deployment
- Establish clear learning expectations and outcomes for the complete course
- Recognize career opportunities and skill requirements in the Edge AI field

### Study Focus Areas

#### Section 1: Edge AI Paradigm and Definition
- **Priority Concepts**: 
  - Edge AI vs. traditional cloud AI processing
  - The convergence of hardware, model optimization, and business demands
  - Real-time, privacy-preserving, and cost-efficient AI deployment

#### Section 2: Industry Applications
- **Priority Concepts**: 
  - Manufacturing & Industry 4.0: Predictive maintenance and quality control
  - Healthcare: Diagnostic imaging and patient monitoring
  - Autonomous Systems: Self-driving vehicles and transportation
  - Smart Cities: Traffic management and public safety
  - Consumer Technology: Smartphones, wearables, and smart homes

#### Section 3: Small Language Models Foundation
- **Priority Concepts**: 
  - SLM characteristics and performance comparisons
  - Parameter efficiency vs. capability trade-offs
  - Edge deployment constraints and optimization strategies

#### Section 4: Learning Framework and Career Path
- **Priority Concepts**: 
  - Course architecture and progressive mastery approach
  - Technical skills and practical implementation goals
  - Career advancement opportunities and industry applications

### Self-Assessment Questions

1. What are the three main technological trends that have enabled Edge AI?
2. Compare the advantages and challenges of Edge AI vs. cloud-based AI.
3. Name three industries where Edge AI provides critical business value and explain why.
4. How do Small Language Models make Edge AI practical for real-world deployment?
5. What are the key technical skills you'll develop throughout this course?
6. Describe the four-phase learning approach used in this course.

### Hands-on Exercises

1. **Industry Research**: Choose one industry application and research a real-world Edge AI implementation (30 minutes)
2. **Model Exploration**: Browse available Small Language Models on Hugging Face and compare their parameter counts and capabilities (30 minutes)
3. **Learning Planning**: Review the complete course structure and create your personal study schedule (15 minutes)

### Supplementary Materials

- [Edge AI Market Overview - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)
- [Small Language Models Overview - Hugging Face](https://huggingface.co/blog/small-language-models)
- [Edge Computing Foundation](https://www.edgecomputing.org/)

## Module 1: EdgeAI Fundamentals and Transformation

### Key Learning Objectives

- Understand the differences between cloud-based and edge-based AI
- Master core optimization techniques for resource-constrained environments
- Analyze real-world applications of EdgeAI technologies
- Set up a development environment for EdgeAI projects

### Study Focus Areas

#### Section 1: EdgeAI Fundamentals
- **Priority Concepts**: 
  - Edge vs. Cloud computing paradigms
  - Model quantization techniques
  - Hardware acceleration options (NPUs, GPUs, CPUs)
  - Privacy and security advantages

- **Supplementary Materials**:
  - [TensorFlow Lite Documentation](https://www.tensorflow.org/lite)
  - [ONNX Runtime GitHub](https://github.com/microsoft/onnxruntime)
  - [Edge Impulse Documentation](https://docs.edgeimpulse.com)

#### Section 2: Real-World Case Studies
- **Priority Concepts**: 
  - Microsoft Phi & Mu model ecosystem
  - Practical implementations across industries
  - Deployment considerations

#### Section 3: Practical Implementation Guide
- **Priority Concepts**: 
  - Development environment setup
  - Quantization and optimization tools
  - Assessment methods for EdgeAI implementations

#### Section 4: Edge Deployment Hardware
- **Priority Concepts**: 
  - Hardware platform comparisons
  - Optimization strategies for specific hardware
  - Deployment considerations

### Self-Assessment Questions

1. Compare and contrast cloud-based AI with edge-based AI implementations.
2. Explain three key techniques for optimizing models for edge deployment.
3. What are the primary advantages of running AI models at the edge?
4. Describe the process of quantizing a model and how it affects performance.
5. Explain how different hardware accelerators (NPUs, GPUs, CPUs) influence EdgeAI deployment.

### Hands-on Exercises

1. **Quick Environment Setup**: Configure a minimal development environment with the essential packages (30 minutes)
2. **Model Exploration**: Download and examine a pre-trained small language model (1 hour)
3. **Basic Quantization**: Try simple quantization on a small model (1 hour)

## Module 2: Small Language Model Foundations

### Key Learning Objectives

- Understand the architectural principles of different SLM families
- Compare model capabilities across different parameter scales
- Evaluate models based on efficiency, capability, and deployment requirements
- Recognize appropriate use cases for different model families

### Study Focus Areas

#### Section 1: Microsoft Phi Model Family
- **Priority Concepts**: 
  - Design philosophy evolution
  - Efficiency-first architecture
  - Specialized capabilities

#### Section 2: Qwen Family
- **Priority Concepts**: 
  - Open source contributions
  - Scalable deployment options
  - Advanced reasoning architecture

#### Section 3: Gemma Family
- **Priority Concepts**: 
  - Research-driven innovation
  - Multimodal capabilities
  - Mobile optimization

#### Section 4: BitNET Family
- **Priority Concepts**: 
  - 1-bit quantization technology
  - Inference optimization framework
  - Sustainability considerations

#### Section 5: Microsoft Mu Model
- **Priority Concepts**: 
  - Device-first architecture
  - System integration with Windows
  - Privacy-preserving operation

#### Section 6: Phi-Silica
- **Priority Concepts**: 
  - NPU-optimized architecture
  - Performance metrics
  - Developer integration

### Self-Assessment Questions

1. Compare the architectural approaches of the Phi and Qwen model families.
2. Explain how BitNET's quantization technology differs from traditional quantization.
3. What are the unique advantages of the Mu model for Windows integration?
4. Describe how Phi-Silica leverages NPU hardware for performance optimization.
5. For a mobile application with limited connectivity, which model family would be most appropriate and why?

### Hands-on Exercises

1. **Model Comparison**: Quick benchmark of two different SLM models (1 hour)
2. **Simple Text Generation**: Basic implementation of text generation with a small model (1 hour)
3. **Fast Optimization**: Apply one optimization technique to improve inference speed (1 hour)

## Module 3: Small Language Model Deployment

### Key Learning Objectives

- Select appropriate models based on deployment constraints
- Master optimization techniques for various deployment scenarios
- Implement SLMs in both local and cloud environments
- Design production-ready configurations for EdgeAI applications

### Study Focus Areas

#### Section 1: SLM Advanced Learning
- **Priority Concepts**: 
  - Parameter classification framework
  - Advanced optimization techniques
  - Model acquisition strategies

#### Section 2: Local Environment Deployment
- **Priority Concepts**: 
  - Ollama platform deployment
  - Microsoft Foundry local solutions
  - Framework comparative analysis

#### Section 3: Containerized Cloud Deployment
- **Priority Concepts**: 
  - vLLM high-performance inference
  - Container orchestration
  - ONNX Runtime implementation

### Self-Assessment Questions

1. What factors should be considered when selecting between local deployment and cloud deployment?
2. Compare Ollama and Microsoft Foundry Local as deployment options.
3. Explain the benefits of containerization for SLM deployment.
4. What are the key performance metrics to monitor for an edge-deployed SLM?
5. Describe a complete deployment workflow from model selection to production implementation.

### Hands-on Exercises

1. **Basic Local Deployment**: Deploy a simple SLM using Ollama (1 hour)
2. **Performance Check**: Run a quick benchmark on your deployed model (30 minutes)
3. **Simple Integration**: Create a minimal application that uses your deployed model (1 hour)

## Module 4: Model Format Conversion and Quantization

### Key Learning Objectives

- Master advanced quantization techniques from 1-bit to 8-bit precision
- Understand format conversion strategies (GGUF, ONNX)
- Implement optimization across six frameworks (Llama.cpp, Olive, OpenVINO, MLX, workflow synthesis)
- Deploy optimized models for production edge environments across Intel, Apple, and cross-platform hardware

### Study Focus Areas

#### Section 1: Quantization Foundations
- **Priority Concepts**: 
  - Precision classification framework
  - Performance vs. accuracy trade-offs
  - Memory footprint optimization

#### Section 2: Llama.cpp Implementation
- **Priority Concepts**: 
  - Cross-platform deployment
  - GGUF format optimization
  - Hardware acceleration techniques

#### Section 3: Microsoft Olive Suite
- **Priority Concepts**: 
  - Hardware-aware optimization
  - Enterprise-grade deployment
  - Automated optimization workflows

#### Section 4: OpenVINO Toolkit
- **Priority Concepts**: 
  - Intel hardware optimization
  - Neural Network Compression Framework (NNCF)
  - Cross-platform inference deployment
  - OpenVINO GenAI for LLM deployment

#### Section 5: Apple MLX Framework
- **Priority Concepts**: 
  - Apple Silicon optimization
  - Unified memory architecture
  - LoRA fine-tuning capabilities

#### Section 6: Edge AI Development Workflow Synthesis
- **Priority Concepts**: 
  - Unified workflow architecture
  - Framework selection decision trees
  - Production readiness validation
  - Future-proofing strategies

### Self-Assessment Questions

1. Compare quantization strategies across different precision levels (1-bit to 8-bit).
2. Explain the advantages of GGUF format for edge deployment.
3. How does hardware-aware optimization in Microsoft Olive improve deployment efficiency?
4. What are the key benefits of OpenVINO's NNCF for model compression?
5. Describe how Apple MLX leverages unified memory architecture for optimization.
6. How does workflow synthesis help in selecting optimal optimization frameworks?

### Hands-on Exercises

1. **Model Quantization**: Apply different quantization levels to a model and compare results (1 hour)
2. **OpenVINO Optimization**: Use NNCF to compress a model for Intel hardware (1 hour)
3. **Framework Comparison**: Test the same model across three different optimization frameworks (1 hour)
4. **Performance Benchmarking**: Measure optimization impact on inference speed and memory usage (1 hour)

## Module 5: SLMOps - Small Language Model Operations

### Key Learning Objectives

- Understand SLMOps lifecycle management principles
- Master distillation and fine-tuning techniques for edge deployment
- Implement production deployment strategies with monitoring
- Build enterprise-grade SLM operations and maintenance workflows

### Study Focus Areas

#### Section 1: Introduction to SLMOps
- **Priority Concepts**: 
  - SLMOps paradigm shift in AI operations
  - Cost efficiency and privacy-first architecture
  - Strategic business impact and competitive advantages

#### Section 2: Model Distillation
- **Priority Concepts**: 
  - Knowledge transfer techniques
  - Two-stage distillation process implementation
  - Azure ML distillation workflows

#### Section 3: Fine-tuning Strategies
- **Priority Concepts**: 
  - Parameter-efficient fine-tuning (PEFT)
  - LoRA and QLoRA advanced methods
  - Multi-adapter training and hyperparameter optimization

#### Section 4: Production Deployment
- **Priority Concepts**: 
  - Model conversion and quantization for production
  - Foundry Local deployment configuration
  - Performance benchmarking and quality validation

### Self-Assessment Questions

1. How does SLMOps differ from traditional MLOps?
2. Explain the benefits of model distillation for edge deployment.
3. What are the key considerations for fine-tuning SLMs in resource-constrained environments?
4. Describe a complete production deployment pipeline for edge AI applications.

### Hands-on Exercises

1. **Basic Distillation**: Create a smaller model from a larger teacher model (1 hour)
2. **Fine-tuning Experiment**: Fine-tune a model for a specific domain (1 hour)
3. **Deployment Pipeline**: Set up a basic CI/CD pipeline for model deployment (1 hour)

## Module 6: SLM Agentic Systems - AI Agents and Function Calling

### Key Learning Objectives

- Build intelligent AI agents for edge environments using Small Language Models
- Implement function calling capabilities with systematic workflows
- Master Model Context Protocol (MCP) integration for standardized tool interaction
- Create sophisticated agentic systems with minimal human intervention

### Study Focus Areas

#### Section 1: AI Agents and SLM Foundations
- **Priority Concepts**: 
  - Agent classification framework (reflex, model-based, goal-based, learning agents)
  - SLM vs LLM trade-offs analysis
  - Edge-specific agent design patterns
  - Resource optimization for agents

#### Section 2: Function Calling in Small Language Models
- **Priority Concepts**: 
  - Systematic workflow implementation (intent detection, JSON output, external execution)
  - Platform-specific implementations (Phi-4-mini, selected Qwen models, Microsoft Foundry Local)
  - Advanced examples (multi-agent collaboration, dynamic tool selection)
  - Production considerations (rate limiting, audit logging, security measures)

#### Section 3: Model Context Protocol (MCP) Integration
- **Priority Concepts**: 
  - Protocol architecture and layered system design
  - Multi-backend support (Ollama for development, vLLM for production)
  - Connection protocols (STDIO and SSE modes)
  - Real-world applications (web automation, data processing, API integration)

### Self-Assessment Questions

1. What are the key architectural considerations for edge AI agents?
2. How does function calling enhance agent capabilities?
3. Explain the role of Model Context Protocol in agent communication.

### Hands-on Exercises

1. **Simple Agent**: Build a basic AI agent with function calling (1 hour)
2. **MCP Integration**: Implement MCP in an agent application (30 minutes)

## Workshop: Hands-On Learning Path

### Key Learning Objectives

- Build production-ready AI applications using Foundry Local SDK and best practices
- Implement comprehensive error handling and user feedback patterns
- Create RAG pipelines with quality evaluation and performance monitoring
- Develop multi-agent systems with coordinator patterns
- Master intelligent model routing for task-based model selection
- Deploy local-first AI solutions with privacy-preserving architectures

### Study Focus Areas

#### Session 01: Getting Started with Foundry Local
- **Priority Concepts**:
  - FoundryLocalManager SDK integration and automatic service discovery
  - Basic and streaming chat implementations
  - Error handling patterns and user feedback
  - Environment-based configuration

#### Session 02: Building AI Solutions with RAG
- **Priority Concepts**:
  - In-memory vector embeddings with sentence-transformers
  - RAG pipeline implementation (retrieve → generate)
  - Quality evaluation with RAGAS metrics
  - Import safety for optional dependencies

#### Session 03: Open Source Models
- **Priority Concepts**:
  - Multi-model benchmarking strategies
  - Latency and throughput measurements
  - Graceful degradation and error recovery
  - Performance comparison across model families

#### Session 04: Cutting-Edge Models
- **Priority Concepts**:
  - SLM vs LLM comparison methodology
  - Type hints and comprehensive output formatting
  - Per-model error handling
  - Structured results for analysis

#### Session 05: AI-Powered Agents
- **Priority Concepts**:
  - Multi-agent orchestration with coordinator pattern
  - Agent memory management and state tracking
  - Pipeline error handling and stage logging
  - Performance monitoring and statistics

#### Session 06: Models as Tools
- **Priority Concepts**:
  - Intent detection and pattern matching
  - Keyword-based model routing algorithms
  - Multi-step pipelines (plan → execute → refine)
  - Comprehensive function documentation

### Self-Assessment Questions

1. How does FoundryLocalManager simplify service management compared to manual REST calls?
2. Explain the importance of import guards for optional dependencies like sentence-transformers.
3. What strategies ensure graceful degradation in multi-model benchmarking?
4. How does the coordinator pattern orchestrate multiple specialist agents?
5. Describe the components of an intelligent model router.
6. What are the key elements of production-ready error handling?

### Hands-on Exercises

1. **Chat Application**: Implement streaming chat with error handling (45 minutes)
2. **RAG Pipeline**: Build minimal RAG with quality evaluation (1 hour)
3. **Model Benchmarking**: Compare 3+ models on performance (1 hour)
4. **Multi-Agent System**: Create coordinator with 2 specialist agents (1.5 hours)
5. **Intelligent Router**: Build task-based model selection (1 hour)
6. **Production Deployment**: Add monitoring and comprehensive error handling (45 minutes)

### Time Allocation

**Concentrated Learning (1 week)**:
- Day 1: Session 01-02 (Chat + RAG) - 3 hours
- Day 2: Session 03-04 (Benchmarking + Comparison) - 3 hours
- Day 3: Session 05-06 (Agents + Routing) - 3 hours
- Day 4: Hands-on exercises and validation - 2 hours

**Part-time Study (2 weeks)**:
- Week 1: Sessions 01-03 (6 hours total)
- Week 2: Sessions 04-06 + exercises (5 hours total)

## Module 7: EdgeAI Implementation Samples

### Key Learning Objectives

- Master AI Toolkit for Visual Studio Code for comprehensive EdgeAI development workflows
- Gain expertise in Windows AI Foundry platform and NPU optimization strategies
- Implement EdgeAI across multiple hardware platforms and deployment scenarios
- Build production-ready EdgeAI applications with platform-specific optimizations

### Study Focus Areas

#### Section 1: AI Toolkit for Visual Studio Code
- **Priority Concepts**: 
  - Comprehensive Edge AI development environment within VS Code
  - Model catalog and discovery for edge deployment
  - Local testing, optimization, and agent development workflows
  - Performance monitoring and evaluation for edge scenarios

#### Section 2: Windows EdgeAI Development Guide
- **Priority Concepts**: 
  - Windows AI Foundry platform comprehensive overview
  - Phi Silica API for efficient NPU inference
  - Computer Vision APIs for image processing and OCR
  - Foundry Local CLI for local development and testing

#### Section 3: Platform-Specific Implementations
- **Priority Concepts**: 
  - NVIDIA Jetson Orin Nano deployment (67 TOPS AI performance)
  - Mobile applications with .NET MAUI and ONNX Runtime GenAI
  - Azure EdgeAI solutions with cloud-edge hybrid architecture
  - Windows ML optimization with universal hardware support
  - Foundry Local applications with privacy-focused RAG implementation

### Self-Assessment Questions

1. How does AI Toolkit streamline the EdgeAI development workflow?
2. Compare deployment strategies across different hardware platforms.
3. What are the advantages of Windows AI Foundry for edge development?
4. Explain the role of NPU optimization in modern edge AI applications.
5. How does the Phi Silica API leverage NPU hardware for performance optimization?
6. Compare the benefits of local vs. cloud deployment for privacy-sensitive applications.

### Hands-on Exercises

1. **AI Toolkit Setup**: Configure AI Toolkit and optimize a model (1 hour)
2. **Windows AI Foundry**: Build a simple Windows AI application using Phi Silica API (1 hour)
3. **Cross-Platform Deployment**: Deploy the same model on two different platforms (1 hour)
4. **NPU Optimization**: Test NPU performance with Windows AI Foundry tools (30 minutes)

## Module 8: Microsoft Foundry Local – Complete Developer Toolkit (Modernized)

### Key Learning Objectives

- Install and configure Foundry Local with modern SDK integration
- Implement advanced multi-agent systems with coordinator patterns
- Build intelligent model routers with automatic task-based selection
- Deploy production-ready AI solutions with comprehensive monitoring
- Integrate with Azure AI Foundry for hybrid deployment scenarios
- Master modern SDK patterns with FoundryLocalManager and OpenAI client

### Study Focus Areas

#### Section 1: Modern Installation and Configuration
- **Priority Concepts**: 
  - FoundryLocalManager SDK integration
  - Automatic service discovery and health monitoring
  - Environment-based configuration patterns
  - Production deployment considerations

#### Section 2: Advanced Multi-Agent Systems
- **Priority Concepts**: 
  - Coordinator pattern with specialist agents
  - Retrieval, reasoning, and execution agent specialization
  - Feedback loop mechanisms for refinement
  - Performance monitoring and statistics tracking

#### Section 3: Intelligent Model Routing
- **Priority Concepts**: 
  - Keyword-based model selection algorithms
  - Multiple model support (general, reasoning, code, creative)
  - Environment variable configuration for flexibility
  - Service health checking and error handling

#### Section 4: Production-Ready Implementation
- **Priority Concepts**: 
  - Comprehensive error handling and fallback mechanisms
  - Request monitoring and performance tracking
  - Interactive Jupyter notebook examples with benchmarks
  - Integration patterns with existing applications

### Self-Assessment Questions

1. How does the modern FoundryLocalManager approach differ from manual REST calls?
2. Explain the coordinator pattern and how it orchestrates specialist agents.
3. How does the intelligent router select appropriate models based on query content?
4. What are the key components of a production-ready AI agent system?
5. How do you implement comprehensive health monitoring for Foundry Local services?
6. Compare the benefits of the modernized approach vs. traditional implementation patterns.

### Hands-on Exercises

1. **Modern SDK Setup**: Configure FoundryLocalManager with automatic service discovery (30 minutes)
2. **Multi-Agent System**: Run the advanced coordinator with specialist agents (30 minutes)
3. **Intelligent Routing**: Test the model router with different query types (30 minutes)
4. **Interactive Exploration**: Use the Jupyter notebooks to explore advanced features (45 minutes)
5. **Production Deployment**: Implement monitoring and error handling patterns (30 minutes)
6. **Hybrid Integration**: Configure Azure AI Foundry fallback scenarios (30 minutes)


## Time Allocation Guide

To help you make the most of the extended 30-hour course timeline (including Workshop), here's a suggested breakdown of how to allocate your time:

| Activity | Time Allocation | Description |
|----------|----------------|-------------|
| Reading Core Materials | 12 hours | Focusing on the essential concepts in each module |
| Hands-on Exercises | 10 hours | Practical implementation of key techniques (including Workshop) |
| Self-Assessment | 3 hours | Testing your understanding through questions and reflection |
| Mini-Project | 5 hours | Applying knowledge to a small practical implementation |

### Key Focus Areas by Time Constraint

**If you only have 10 hours:**
- Complete Module 0 (Introduction) and Modules 1, 2, and 3 (core EdgeAI concepts)
- Do at least one hands-on exercise per module
- Focus on understanding the core concepts rather than implementation details

**If you can dedicate the full 20 hours:**
- Complete all eight modules (including Introduction)
- Perform key hands-on exercises from each module
- Complete one mini-project from Module 7
- Explore at least 2-3 supplementary resources

**If you have more than 20 hours:**
- Complete all modules (including Introduction) with detailed exercises
- Build multiple mini-projects
- Explore advanced optimization techniques in Module 4
- Implement production deployment from Module 5

## Essential Resources

These carefully selected resources provide the most value for your limited study time:

### Must-Read Documentation
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - The most efficient model optimization tool
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Fastest way to deploy SLMs locally
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Reference for a leading edge-optimized model
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Intel's comprehensive optimization toolkit
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Integrated EdgeAI development environment
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Windows-specific EdgeAI development platform

### Time-Saving Tools
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Quick model access and deployment
- [Gradio](https://www.gradio.app/docs/interface) - Rapid UI development for AI demos
- [Microsoft Olive](https://github.com/microsoft/Olive) - Simplified model optimization
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Efficient CPU inference
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Neural network compression framework
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Large language model deployment toolkit

## Progress Tracking Template

Use this simplified template to track your learning progress through the 20-hour course:

| Module | Completion Date | Hours Spent | Key Takeaways |
|--------|----------------|-------------|---------------|
| Module 0: Introduction to EdgeAI | | | |
| Module 1: EdgeAI Fundamentals | | | |
| Module 2: SLM Foundations | | | |
| Module 3: SLM Deployment | | | |
| Module 4: Model Optimization | | | |
| Module 5: SLMOps | | | |
| Module 6: AI Agents | | | |
| Module 7: Development Tools | | | |
| Workshop: Hands-On Learning | | | |
| Module 8: Foundry Local Toolkit | | | |
| Hands-on Exercises | | | |
| Mini-Project | | | |

## Mini Project Ideas

Consider completing one of these projects to practice EdgeAI concepts (each designed to take 2-4 hours):

### Beginner Projects (2-3 hours each)
1. **Edge Text Assistant**: Create a simple offline text completion tool using a small language model
2. **Model Comparison Dashboard**: Build a basic visualization of performance metrics across different SLMs
3. **Optimization Experiment**: Measure the impact of different quantization levels on the same base model

### Intermediate Projects (3-4 hours each)
4. **AI Toolkit Workflow**: Use VS Code AI Toolkit to optimize and deploy a model from start to finish
5. **Windows AI Foundry Application**: Create a Windows app using Phi Silica API and NPU optimization
6. **Cross-Platform Deployment**: Deploy the same optimized model on Windows (OpenVINO) and mobile (.NET MAUI)
7. **Function Calling Agent**: Build an AI agent with function calling capabilities for edge scenarios

### Advanced Integration Projects (4-5 hours each)
8. **OpenVINO Optimization Pipeline**: Implement complete model optimization using NNCF and GenAI toolkit
9. **SLMOps Pipeline**: Implement a complete model lifecycle from training to edge deployment
10. **Multi-Model Edge System**: Deploy multiple specialized models working together on edge hardware
11. **MCP Integration System**: Build an agentic system using Model Context Protocol for tool interaction

## References

- Microsoft Learn (Foundry Local)
  - Overview: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Get started: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - CLI reference: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Integrate with inference SDKs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Open WebUI how-to: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Compile Hugging Face models: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Overview: https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agents (overview): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Optimization and Inference Tooling
  - Microsoft Olive (docs): https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive
  - ONNX Runtime (getting started): https://onnxruntime.ai/docs/get-started/with-python.html
  - ONNX Runtime Olive integration: https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (docs): https://docs.openvino.ai/2025/index.html
  - Apple MLX (docs): https://ml-explore.github.io/mlx/build/html/index.html
- Deployment Frameworks and Models
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index
  - vLLM (docs): https://docs.vllm.ai/
  - Ollama (quick start): https://github.com/ollama/ollama#get-started
- Developer Tools (Windows and VS Code)
  - AI Toolkit for VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (overview): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Learning Community

Join the discussion and connect with fellow learners:
- GitHub Discussions on the [EdgeAI for Beginners repository](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Conclusion

EdgeAI represents the frontier of artificial intelligence implementation, bringing powerful capabilities directly to devices while addressing critical concerns about privacy, latency, and connectivity. This 20-hour course provides you with the essential knowledge and practical skills to begin working with EdgeAI technologies immediately.

The course is deliberately concise and focused on the most important concepts, allowing you to quickly gain valuable expertise without an overwhelming time commitment. Remember that hands-on practice, even with simple examples, is the key to reinforcing what you've learned.

Happy learning!
